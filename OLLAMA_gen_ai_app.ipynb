{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: ipykernel in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (6.23.1)\n",
      "Requirement already satisfied: appnope in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (8.12.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (6.3.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: backcall in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (6.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "#Langsmith Tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "\n",
    "##Data Ingestion -- From the website we need to scrap the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the data to chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/682zxx4166d4gj9yjn9j6mc00000gn/T/ipykernel_5082/3902904122.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/bibhukalyanojha/tensorflow-test/env/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84e53791454b74b4a5aa97f6d912cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb8a3162bf94014b1eb063a72e3b1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91e2a03ead14ad292f74bba3fc6e6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09aaca02ebb4062abc517442b9ca49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e4599cb66244acb7ad41b8b58e86f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c056ee2006c4110b3406ead95eedec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82af7bb5e6e4c6ba4439588c0cfe705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cd52aba0364e9b8bd24f5c5106be44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ccc554fd03421cb7a4d08b61f73a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c11dfdd5b44979854abac277c9e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef694ba64e14d9e9e9be81a70785e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Embeded text to vectors\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "## Store data to FAISS db\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query from vector stored DB\n",
    "result=db.similarity_search(\"Subgoal and decomposition\")\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrival chain, Document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "import streamlit as st\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "##LLAMA3 model\n",
    "llm=Ollama(model=\"LLAMA3.2\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Document chain\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "#Prompt template\n",
    "# prompt=ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\",\"You are a helpful assistant. Please respond to the question asked.\"),\n",
    "#         (\"user\",\"Question:{question}\")\n",
    "#     ]\n",
    "# )\n",
    "##output parser\n",
    "#output_parser=StrOutputParser()\n",
    "## Document chain\n",
    "#document_chain = prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help clarify subgoals and decomposition.\\n\\n**What is Subgoal and Decomposition?**\\n\\nSubgoals and decomposition are two fundamental concepts in problem-solving, planning, and decision-making. They are used to break down a complex problem into smaller, more manageable tasks (subgoals) that can be achieved one by one.\\n\\n**Subgoal:**\\nA subgoal is a specific, achievable goal that contributes to the overall objective of a task or project. It's a smaller, more concrete target that can be accomplished by focusing on a particular aspect of the larger problem. Subgoals are often used to create a roadmap for achieving a higher-level goal.\\n\\n**Decomposition:**\\nDecomposition is the process of breaking down a complex problem into its constituent parts, identifying subgoals, and creating a plan to achieve each one. It involves dividing the original problem into smaller, more manageable chunks, making it easier to understand, analyze, and solve.\\n\\n**Benefits of Subgoal and Decomposition:**\\n\\n1. **Improved clarity**: Breaks down complex problems into smaller, more understandable components.\\n2. **Increased efficiency**: Allows for more focused attention on individual subgoals, reducing the likelihood of getting bogged down in details.\\n3. **Enhanced decision-making**: Provides a clear roadmap for achieving objectives, enabling better-informed decisions.\\n\\n**Example:**\\nSuppose you want to plan a road trip from New York to Los Angeles (OL). You can break it down into smaller subgoals:\\n\\n1. Plan the route from New York to Chicago.\\n2. Arrange accommodations in Chicago.\\n3. Drive from Chicago to Denver.\\n4. Continue driving through the Rocky Mountains.\\n5. Reach Los Angeles.\\n\\nEach subgoal represents a specific, achievable task that contributes to achieving the overall objective of reaching Los Angeles.\\n\\nDo you have any specific questions about subgoals and decomposition or would you like me to elaborate on this topic further?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=document_chain.invoke({\"question\":\"Subgoal and decomposition\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(...)\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x3c16a61f0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '))])\n",
       "            | Ollama(model='LLAMA3.2')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input --> Retriver --> vectorstoredb\n",
    "retriver=db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrival_chain=create_retrieval_chain(retriver, document_chain)\n",
    "retrival_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Subgoal and decomposition',\n",
       " 'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Planning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Task Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')],\n",
       " 'answer': 'According to the provided context, human agents can decompose tasks into smaller subgoals by using one of three methods:\\n\\n1. LLM (Large Language Model) with simple prompting, such as \"Steps for XYZ.\\\\n1.\", or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, e.g., \"Write a story outline.\" for writing a novel.\\n3. With human input.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n"
     ]
    }
   ],
   "source": [
    "## Get the response from LLM\n",
    "resonse=retrival_chain.invoke({\"input\":\"Subgoal and decomposition\"})\n",
    "resonse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, human agents can decompose tasks into smaller subgoals by using one of three methods:\\n\\n1. LLM (Large Language Model) with simple prompting, such as \"Steps for XYZ.\\\\n1.\", or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, e.g., \"Write a story outline.\" for writing a novel.\\n3. With human input.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resonse[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
